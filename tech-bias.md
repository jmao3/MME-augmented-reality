The technology of augmented reality has a strong bias toward vision, because humans, who invented AR, obtain most of outside information through our eyes. In the book on AR I am reading, the author has one whole chapter just to explain the mechanism of human eyes. Also, most AR products nowadays take form of goggles. However, the wider definition of AR should include more than vision. In its essence, AR is the overlay of information over human perception, which can include all human senses like auditory or haptic senses. Our natural bias toward vision leads to limited researches on other AR-possible senses.
For example, stereo sounds, which contain more layers of information (like the distance and the direction) than the literal message has, provides an auditory AR experience to some degree. However, it is almost exclusively used in gaming and movie industries. Other softwares and applications seem to be completely oblivious of its existence. The sounds they offer are mostly monaural. The auditory clue of stereo sounds contain could potentially be another aspect of human-computer interaction, yet almost none of the softwares takes advantages of them.
Another important sense humans rely on is our haptic sense. It has been so natural to us that we forget how crucial the information it provides us with. There have been attempts to trigger this sense, like the vibration mode of cell phones, but the effort is very limited. The company that explores the most on this is probably Apple, which has features like 3D Touch and Taptic Engines on iPhones. Nevertheless, the experience is way too subtle. What we feel through our haptic sense includes textures, weights, temperatures, and etc.. It is much more than a little vibration. Yet because our bias toward vision, we get very limited experience from haptic AR.
